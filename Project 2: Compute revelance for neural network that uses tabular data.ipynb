{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c076f23",
   "metadata": {},
   "source": [
    "## Compute revelance for neural network that uses tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cde2ff",
   "metadata": {},
   "source": [
    "Here we demonstrate how we can calculate the layer wise relevence of a neural network which can provide insights on important features that contribute to the target variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchvision\n",
    "#import torchvision.models as models\n",
    "#import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim import Adam\n",
    "from IPython import display\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dat_nc=pd.read_csv(\"/Users/seymour/Desktop/SuStaIn_15_June/Andre_sent_21_July/img_dat_nc.csv\")\n",
    "img_dat_nc[\"SubjID\"] = img_dat_nc[\"Site\"] + img_dat_nc[\"SubjID\"].astype(str)+ img_dat_nc[\"SDx_dti\"].astype(str)\n",
    "img_dat_nc[\"SubjID\"] = img_dat_nc[\"SubjID\"].str[:-2]\n",
    "img_dat_nc[\"SDx_dti\"] = img_dat_nc[\"SDx_dti\"].map({3: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70066e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_feats=['Lhippo','Rhippo','Lamyg','Ramyg','LLatVent','RLatVent','Lthal','Rthal','Lcaud','Rcaud','Lput','Rput','Lpal','Rpal','Laccumb','Raccumb','L_frontalL_volume','L_parietalL_volume','L_temporalL_volume','L_occipitalL_volume','L_cingulateC_volume','R_frontalL_volume','R_parietalL_volume','R_temporalL_volume','R_occipitalL_volume','R_cingulateC_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff066f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=img_dat_nc[t1_feats]\n",
    "X_normalized=normalize(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, img_dat_nc.SDx_dti, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cdd5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change data type to match pytorch tensor format\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [50, 64]\n",
    "output_size = 2\n",
    "\n",
    "# Define the model\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                  ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                  ('relu1', nn.ReLU()),\n",
    "                  ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                  ('relu2', nn.ReLU()),\n",
    "                  ('fc_output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                  ('softmax', nn.Softmax(dim=1))]))\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "y_train = torch.from_numpy(y_train.values)\n",
    "y_train = y_train.long()\n",
    "\n",
    "num_epochs=10\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    output = model(X_train)\n",
    "    output = output.float()\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    test_output = model(X_test)\n",
    "    test_predictions = torch.sigmoid(test_output)\n",
    "    #test_predictions=test_predictions.round()\n",
    "\n",
    "# Calculate AUC metric\n",
    "auc = roc_auc_score(y_test, test_predictions[:,0])\n",
    "print(\"AUC: \", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, test_predictions[:,0].round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc49466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aLminus1=activation of layer l-1\n",
    "#Rj= last R_nextk\n",
    "\n",
    "def lrp(model, x,pred):\n",
    "    model.eval()\n",
    "    xj=x\n",
    "    namej=\"fc1\"\n",
    "    length=len(list(model.modules()))\n",
    "    print(length)\n",
    "    \n",
    "    for i, (name, layer) in enumerate(model._modules.items()):\n",
    "        print(f\"{name}\")\n",
    "\n",
    "#         print(f\"Layer {i+1}:{name}\")\n",
    "#         print(layer)\n",
    "        \n",
    "        if i==0:\n",
    "            out = model._modules[f\"{name}\"](x)\n",
    "            w = model.state_dict()[f\"{name}.weight\"]\n",
    "            Rk = out.data\n",
    "            aLminus1 = x.data\n",
    "            z = aLminus1.unsqueeze(0).numpy().dot(w.T.numpy()) #Here summation of aj.wk is done via matrix dot product\n",
    "            s = Rk/z\n",
    "            c = s.numpy().dot(w.numpy())\n",
    "            R_nextk=c\n",
    "\n",
    "               \n",
    "            \n",
    "        if i>0:\n",
    "            print(type(model._modules[name]))\n",
    "\n",
    "            if isinstance(model._modules[name], torch.nn.Conv2d) or\\\n",
    "            isinstance(model._modules[name], torch.nn.AvgPool2d) or\\\n",
    "            isinstance(model._modules[name], torch.nn.Linear):\n",
    "                print(f\"{name}\")\n",
    "                \n",
    "                if name=='fc_output':\n",
    "                    outj = pred\n",
    "                else:outj = model._modules[f\"{name}\"](out)\n",
    "   \n",
    "                \n",
    "                wj = model.state_dict()[f\"{name}.weight\"]\n",
    "                Rk = R_nextk\n",
    "                aLminus1 = out.data #out is the previous layer activation\n",
    "                z = aLminus1.unsqueeze(0).numpy().dot(wj.T.numpy())\n",
    "                z=z.T\n",
    "                s = Rk/z\n",
    "                cj = s.T.dot(wj.numpy()) \n",
    "                sum_cj=np.sum(cj,axis=1) \n",
    "\n",
    "                #update parameters for next iteration\n",
    "                out=outj\n",
    "                R_nextk=sum_cj\n",
    "                \n",
    "    return R_nextk,out\n",
    "\n",
    "relevance,pred1=lrp(model, X_test[0,:],test_predictions)\n",
    "relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4746f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
